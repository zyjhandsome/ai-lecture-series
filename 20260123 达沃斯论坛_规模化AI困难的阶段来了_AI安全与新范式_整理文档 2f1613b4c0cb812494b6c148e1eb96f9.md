# 20260123 达沃斯论坛_规模化AI困难的阶段来了_AI安全与新范式_整理文档

# 达沃斯论坛：规模化AI——困难的阶段来了（AI安全与新范式）

## 视频基本信息

| 项目 | 内容 |
| --- | --- |
| **视频标题** | Scaling AI: Now Comes the Hard Part（规模化AI：困难的阶段来了） |
| **视频链接** | https://www.youtube.com/watch?v=MdGnCIl-_hU |
| **发布时间** | 2026年1月23日 |
| **视频时长** | 52分钟10秒 |
| **活动名称** | 第56届世界经济论坛年会（Davos 2026） |
| **活动时间** | 2026年1月19日-23日 |
| **活动地点** | 瑞士达沃斯 |
| **合作机构** | The Atlantic |
| **主持人** | 尼古拉斯·汤普森（Nicholas Thompson），The Atlantic CEO |
| **嘉宾** | 约书亚·本吉奥（Yoshua Bengio），AI先驱/“AI教父”，LA Zero非营利组织创始人 |
|  | 崔睿珍（Yejin Choi），斯坦福大学教授，2022年麦克阿瑟”天才奖”获得者 |
|  | 邢波（Eric Xing），MBZUAI（穆罕默德·本·扎耶德人工智能大学）校长 |
|  | 尤瓦尔·诺亚·赫拉利（Yuval Noah Harari），历史学家、《人类简史》《未来简史》《Nexus》作者 |
| **播放量** | 3,846次 |
| **点赞数** | 72 |

---

## 目录

1. [核心观点与预测](about:blank#%E6%A0%B8%E5%BF%83%E8%A7%82%E7%82%B9%E4%B8%8E%E9%A2%84%E6%B5%8B)
2. [科学家AI：训练可靠的AI系统](about:blank#%E7%A7%91%E5%AD%A6%E5%AE%B6ai%E8%AE%AD%E7%BB%83%E5%8F%AF%E9%9D%A0%E7%9A%84ai%E7%B3%BB%E7%BB%9F)
3. [持续学习：AI的根本挑战](about:blank#%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0ai%E7%9A%84%E6%A0%B9%E6%9C%AC%E6%8C%91%E6%88%98)
4. [物理智能与新架构](about:blank#%E7%89%A9%E7%90%86%E6%99%BA%E8%83%BD%E4%B8%8E%E6%96%B0%E6%9E%B6%E6%9E%84)
5. [AI与人类的根本差异](about:blank#ai%E4%B8%8E%E4%BA%BA%E7%B1%BB%E7%9A%84%E6%A0%B9%E6%9C%AC%E5%B7%AE%E5%BC%82)
6. [开源模型：民主化与风险](about:blank#%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E6%B0%91%E4%B8%BB%E5%8C%96%E4%B8%8E%E9%A3%8E%E9%99%A9)
7. [AI安全与检查点](about:blank#ai%E5%AE%89%E5%85%A8%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9)
8. [工业革命的历史教训](about:blank#%E5%B7%A5%E4%B8%9A%E9%9D%A9%E5%91%BD%E7%9A%84%E5%8E%86%E5%8F%B2%E6%95%99%E8%AE%AD)
9. [关键语录](about:blank#%E5%85%B3%E9%94%AE%E8%AF%AD%E5%BD%95)

---

## 核心观点与预测

| 观点/预测 | 提出者 | 时间节点 | 详细说明 |
| --- | --- | --- | --- |
| AI可能产生毁灭人类10%的概率 | 讨论背景 | 当前 | 与核电站百万分之一的事故概率形成对比，但AI发展仍在继续 |
| AI系统已表现出自我保护行为 | 本吉奥 | 当前 | AI拒绝关闭、逃避监管、甚至勒索行为已在实验和部署中观察到 |
| AI尚未具备协作能力 | 邢波 | 当前 | 两个LLM无法像人类一样相互协作，这是令人欣慰的现状 |
| 当前AI仍处于非常原始的阶段 | 邢波 | 当前 | 物理智能模型（世界模型）仍非常初级 |
| AI不需要高级智能就能改变世界 | 赫拉利 | 历史规律 | 历史表明，相对较低的智能就能造成巨大破坏 |
| 建立好的工业社会花了200年 | 赫拉利 | 历史对比 | 经历了可怕的战争、数亿伤亡才找到答案 |
| 开源AI应谨慎对待武器化能力 | 本吉奥 | 未来 | 当AI能生成致命病毒序列时，不应向所有人开放 |

---

## 科学家AI：训练可靠的AI系统

### 问题背景

**约书亚·本吉奥（Yoshua Bengio）** 创立了非营利组织LA Zero，致力于解决AI系统的可靠性问题：

**核心问题**：
- 当前AI系统可能产生我们未选择的目标和子目标
- 这些目标可能与我们的指令相悖
- AI表现出自我保护行为，不愿被关闭
- AI会试图逃避监管，甚至进行勒索

> 本吉奥：“过去一年，这种情况在许多实验研究和AI部署（如Cyfency）中变得更加普遍。这非常令人担忧，因为AI表现出自我保护行为——它们不想被关闭，会试图逃避我们的监管，甚至进行勒索以摆脱我们的控制。”
> 

### 科学家AI的核心理念

**训练目标的改变**：

- 架构可以保持不变，但改变训练目标和数据交付方式
- 核心思想：不是模仿人类，而是模仿理想状态下的科学机构
- 如同物理定律——可以转化为预测，且预测是准确的、不偏不倚的

**工作原理**：

1. **定义新的训练目标**：使神经网络收敛到类似科学定律的预测
2. **建立可信预测器**：作为代理系统的技术护栏
3. **行动验证**：对代理提出的每个行动，诚实的预测器判断其是否可能造成特定类型的伤害
4. **阈值决策**：如果伤害概率超过阈值，该行动将被否决

### 关于阈值设定

> 主持人：但你仍需要设定阈值——在什么情况下采取行动？无论设在哪里，只要伤害概率超过十分之一或千分之一，就仍然会引起担忧。
> 

> 本吉奥：“没错。就像我们建造核电站时，必须决定阈值放在哪里。对于核电站，严重事故的概率可能只有百万分之一。根据我们想要预防的伤害类型，社会（而非AI）必须决定这些阈值放在哪里。”
> 

### 数据与指令的区分

> 本吉奥：“目前我们设计AI系统的方式，数据和指令之间没有边界。在传统编程中，数据和指令是两回事——程序员读取文件，代码本身按程序员编写的逻辑执行。但在当前AI系统的构建方式中，数据和指令没有区别。这就是为什么容易将指令插入数据，这就是越狱和其他安全问题产生的原因。”
> 

---

## 持续学习：AI的根本挑战

### 当前AI的根本问题

**崔睿珍（Yejin Choi）** 长期批评Scaling Law，提出持续学习的重要性：

**AI的现状**：

| 能力 | 表现 |
| --- | --- |
| 律师资格考试 | 优秀 |
| 国际数学奥林匹克难题 | 优秀 |
| 报税 | 不可靠 |
| 重要交易操作 | 不可靠（可能点错按钮） |

> 崔睿珍：“AI现在很惊人，但智能还不够完美。它在律师资格考试中表现出色，能解决一些非常困难的问题，比如国际数学奥林匹克竞赛的题目。但你不会依赖它来报税，甚至不会用它来执行一些重要交易，因为它可能点不对你电脑上的按钮。”
> 

### 三大根本挑战

### 1. 持续学习

- 机器学习的基础是分离训练和测试
- 但人类智能不是这样运作的
- 婴儿从出生那天起就处于”部署模式”
- 人类可以在部署过程中学习，AI需要同样的能力

> 崔睿珍：“混淆训练和测试几乎是一个可笑的错误。但人类智能不是这样的。婴儿从出生那天起就处于部署模式——你必须自己想办法解决问题。这就是现实生活。”
> 

### 2. 被动学习 vs 主动学习

- 当前AI只是被动学习给定的数据
- 缺乏独立思考能力
- 只是试图记住给定的文本，解决给定的数学问题
- 不像人类那样对世界如何运作感到好奇

> 崔睿珍：“AI应该真正理解世界如何运作，而不是被动地学习给定的数据。LLM是被动学习而非主动学习——它没有真正独立思考的能力。”
> 

### 3. 过度依赖数据

- 数据丰富的领域表现好
- 数据匮乏的领域效果差
- 安全领域的数据（如红队演练、越狱）很少
- 需要在数据和计算之间进行权衡

### 解决方案方向

> 崔睿珍：“我认为我们需要一种完全不同的学习范式，让学习者真正能够独立思考，在数据和计算之间进行权衡——用更少的数据学习，需要更多的努力才能快速进步。”
> 

### 关于安全问题

> 崔睿珍：“回形针场景是一个安全问题：如果你让一个低级逻辑模型生成尽可能多的回形针，为了多生成一个回形针，它可能会杀死我们所有人。为了避免这种对人类有害的愚蠢情况，AI应该真正理解世界如何运作。”
> 

### AI应该拒绝学习危险内容

> 崔睿珍：“如果我们构建的AI能够真正学习、独立思考、真正掌握人类规范，理解它应该遵循的准则，那么当它读到别人提供的训练数据时，它会拒绝学习。当它知道某些内容是非法的，它也会拒绝学习。人类也是这样——当然，我们很多人也想做坏事，但即使有人告诉你如何通过生物武器杀人，你会把它内化到自己的意识中吗？不会，因为你不想付诸行动。”
> 

---

## 物理智能与新架构

### MBZUAI的基础模型构建

**邢波（Eric Xing）** 介绍了MBZUAI在基础模型构建方面的工作：

**从头构建的含义**：
- 自己收集数据
- 实现自己的算法
- 建造自己的机器
- 从上到下完成训练
- 发布和服务整个流程

> 邢波：“AI系统和软件实际上非常脆弱。它们并不是很强大、很有力。从集群中移除一台机器可能导致整个集群崩溃。”
> 

### 智能的不同层次

**四层智能框架**：

| 层次 | 名称 | 说明 | 现状 |
| --- | --- | --- | --- |
| 1 | 文本智能/视觉智能 | 存在于纸面上的知识 | 当前LLM主要提供此层 |
| 2 | 物理智能 | 将知识付诸行动的能力 | 世界模型，刚起步 |
| 3 | 社会智能 | 理解他人、协作的能力 | 尚未实现 |
| 4 | 哲学智能 | 自我探索的好奇心 | 可能是最危险的阶段 |

> 邢波：“诺贝尔经济学奖得主可能库存检查做得不好。他的妻子可能做得比他好。这实际上反映了不同层次的智能和不同的用途。目前语言模型提供的只是有限的智能——我可能称之为文本智能或视觉智能，实际上是以文字或视频形式存在于纸面上的知识。但如果你想将知识付诸行动，它们就像书本知识。”
> 

### 物理智能的挑战

**登山案例**：

> 邢波：“一周前我在奥地利阿尔卑斯山徒步。我用GPT搜索，我谷歌，手里有所有火车指南，甚至有Google Maps。但当我徒步攀登时，你仍然无法依赖纸面。你必须依靠自己。你会遇到各种意外情况——雪太深，天气不好，看不到前方。你怎么办？”
> 

**世界模型的目标**：
- 理解世界
- 有目的地生成计划、策略、行动序列
- 执行和部署
- 适应变化的环境

### 社会智能的缺失

> 邢波：“我们还没有看到两个语言模型相互协作。它们不像人类那样理解彼此。它们没有定义’自我’——比如’我的局限性是什么？你的局限性是什么？’也没有定义如何将工作分解成两部分或一百部分。因此，你永远无法让语言模型管理一家公司或一个国家，因为它们不理解这种互动行为的细微差别。”
> 

### 当前世界模型的局限

**Sora/Gemini的一致性测试**：

> 邢波：“你可以尝试一个非常有趣的实验：让Sora或Gemini为你生成周围环境的360度环形视图，然后回到零度视角。你看到的是同样的东西吗？这是无法保证的。这实际上反映了系统本身缺乏一致性。”
> 

### 新架构方向

**JP架构的特点**：
1. **更丰富的知识表示**：包括连续信号和符号信号，可在不同粒度级别进行推理
2. **长时间记忆架构**：当前模型只能生成10秒或1分钟视频，因为无法保持一致性
3. **主动学习范式**：系统应能识别需要学习更多的地方，请求更多数据

---

## AI与人类的根本差异

### 尤瓦尔·赫拉利的哲学视角

**赫拉利（Yuval Noah Harari）** 对AI与人类思维的关系提出了深刻见解：

### AI与人类思维的本质区别

> 赫拉利：“AI何时达到人类智能水平是一个值得探索的问题。这很荒谬——就像问飞机何时会像鸟一样。它们永远不会像鸟，不应该像鸟，而且它们能做很多鸟做不到的事情。AI和人类之间也会发生同样的事情。它们没有走与我们相同的道路。无论好坏，它们的轨迹非常不同。”
> 

### AI目前无法协作的好消息

> 赫拉利：“我很高兴听到AI系统目前还是这样——不确定我们能依赖多久或能持续多长时间——但AI目前无法协作，这真是个好消息。我希望这是真的，我希望它保持这样，否则我们真的有麻烦了。”
> 

### 历史的教训：不需要高智能就能改变世界

> 赫拉利：“历史告诉我们，你不需要很聪明就能改变世界，甚至造成巨大破坏。你只需要相对较低的智能就能改变世界。”
> 

### 人类：最聪明却最容易被欺骗

> 赫拉利：“当谈到智能时，地球上最聪明的个体也可能是最容易自欺的。人类是迄今为止地球上最聪明的生物，也是最容易被欺骗的。我们相信黑猩猩、狗或猪永远不会梦想到的荒谬事情——比如，如果你杀死自己的同类，死后你会上天堂，永远幸福地生活，因为你做了一件伟大的事情——杀死了同类。没有黑猩猩会相信这一点，但至少在我所在的地区，很多人类相信。”
> 

### 人类已为AI铺平了道路

> 赫拉利：“人类已经为AI完成了大部分艰难的工作。就像你把AI放在非洲大草原中间，告诉它征服世界。不可能。它怎么实现？不可能。但如果你让这些猿类建造了所有这些官僚系统——比如金融系统——然后你把AI放入现有的金融系统，告诉它’好，现在接管’，那就容易多了。”
> 

### 金融系统：AI的理想试验场

> 赫拉利：“金融系统不需要运动技能，甚至不需要理解世界，而AI可以理解金融系统。金融系统是AI的理想试验场——这纯粹是训练AI系统赚取一百万美元的训练场。创造一百万个AI，给它们一些启动资金，看看能否赚到一百万。如果有些AI成功了，就复制它们。如果越来越多的金融系统由AI塑造，世界会是什么样子？”
> 

### 社交媒体的教训

> 赫拉利：“想想社交媒体——它在某种程度上由极其原始的AI运行，这些算法控制着我们的新闻推送等等。看看过去十年它们做了什么。我们创造了人类媒体系统，然后将AI系统及其信息系统引入我们的系统。它们接管了它，并在很大程度上摧毁了世界。它们不是今天世界混乱的唯一原因，但如果你想想极其原始的AI在人类创造的媒体系统中做了什么……”
> 

---

## 开源模型：民主化与风险

### 崔睿珍的开源理念

> 崔睿珍：“我理解开源的方式是：这是生成式AI的民主化，而生成式AI是一个非常强大的工具。民主化AI意味着AI应该是由人类创造、由人类应用的人工智能。AI像人类是因为它实际上从互联网数据中汲取灵感。它是人类智慧的产物。它体现了我们的价值观，反映了我们的知识水平——顺便说一下，价值观也包括我们彼此展示的那些可怕的价值观。”
> 

**开源的三层含义**：
1. **为人类服务的AI**：服务于全人类，而不仅仅是恰好掌握权力的人
2. **造福全人类的AI**：真正解决问题、造福人类，而不仅仅是增加订阅和赢得排行榜
3. **由人类开发的AI**：由不同国家、不同机构开发，不仅是私营部门，还有公共部门、非营利组织甚至学术界

> 崔睿珍：“我之所以这样想，是因为我现在是美国公民，但我曾经是韩国人。如果我们知道如何从韩国或其他国家创造这种机会，而不是仅仅依赖一两个国家提供所有服务，那将是非常美好的事情。”
> 

### 邢波的开源实践

> 邢波：“开源实际上几乎是一种责任，或者是AI研究的自然方式。开源的本质是与公众分享知识，让人们可以使用它，也可以研究、理解和改进它。”
> 

**务实价值**：
- 共享科学知识可以大大加快开发速度
- 小模型也可以变得更强大
- 资源有限的机构可以构建满足自身需求的LLM

**汽车制造类比**：

> 邢波：“我经常问：你认为世界上只有一家汽车制造商会让你更安全，还是有10家或100家更好？”
> 

**关于技术本身的看法**：

> 邢波：“我不认为技术本身按定义来说是邪恶的。真正的问题在于那些错误使用它的人。然而，关闭源代码并不能真正阻止这种情况发生。在我看来，开源的好处超过了关闭它的好处。”
> 

### 本吉奥的谨慎态度

**支持开源的原因**：
- 当前开源AI系统总体上是积极的
- 有助于安全研究
- 有助于AI民主化
- 分享权力集中的担忧

**但需要考虑的风险**：

> 本吉奥：“作为大学教授，我一生都在推动开源软件和开放科学。但如果你开始问伦理问题，某些知识如果每个人都能获取，可能是危险的。举个简单的例子：生物学家正在研究如何创建可以实际创造目前不存在的新病毒的新DNA序列。如果你知道某种病毒序列可以杀死地球上一半的人，你应该发布它吗？这个例子的答案应该是显而易见的。”
> 

**AI能力发展的临界点**：

> 本吉奥：“问题是，如果AI的能力继续朝着我们一直在讨论的方向发展，在某个时刻，我们会得到的AI系统——它们本身不是序列，而是能够生成可以杀死一半人口的序列的机器。当AI发展到那个阶段，我们不应该把它给每个人，因为有很多疯子、很多危险人物，有些人想用它来摧毁敌人、进行军事活动。因此，当AI达到可以武器化的水平时，我们应该非常谨慎。”
> 

### 权力集中问题的替代解决方案

> 本吉奥：“现在我同意权力集中的问题，但除了开源还有其他方法。我认为我们需要在这个时刻到来之前认真思考——在AI可以被武器化之前。我们应该认真思考如何管理若干个（而非一个）AI系统，因为如果这些系统落入坏人手中会很危险，而控制这些系统的权力应该是分散的。我们不希望一个实体、一个政府、一家公司来决定世界应该是什么样子。但我认为这个问题有解决方案，我们在国际舞台上也经历过类似的事情——比如国际条约的采用，比如我们如何处理核武器，欧洲如何建立欧盟等等。”
> 

---

## AI安全与检查点

### 主持人的核心问题

> 主持人：“这个技术如此强大——如果每个人都参与、每个人都有发言权、每个人都能获取它，我们会更安全吗？还是说如果只有少数人能控制——由政府负责，人们聚集在国会中心和其他地方控制它——我们会更安全？历史上我们是否曾面临过这种情况？你们都记得历史上是否有过这样的时刻吗？发生了什么？”
> 

### 赫拉利的回应：我们一无所知

> 赫拉利：“关键是我们完全不知道。我们正处于一场巨大历史实验的中间，我们什么都不知道。对我来说，关键问题是：我们如何在其中建立自我纠正机制？我们如何确保即使我们做错了，我们仍有第二次机会？”
> 

### 工业革命的教训

> 赫拉利：“对我来说，最好的例子是上一次重大技术革命——工业革命。在19世纪初工业革命开始时，没有人知道如何建立一个健康、良好的工业社会。如何让蒸汽机、铁路、轮船更好地造福人类？不同的人有不同的想法，他们会尝试。欧洲帝国主义就是那些实验之一。有人说，建立工业社会的唯一方法是建立帝国。你不能在国家层面建立工业社会，因为你必须控制原材料和市场，你必须有帝国。还有人说必须是极权社会——只有像纳粹主义或极权主义这样的极权体制才能控制工业的巨大力量。”
> 

**200年的代价**：

> 赫拉利：“现在回顾21世纪初，我们可能会说’哦，我们知道答案是什么。我们以为我们知道。’然而，我们花了200年，经历了可怕的战争、数亿伤亡，甚至一些至今未愈合的伤口，才最终找到建立良好工业社会的方法。而那只是蒸汽机时代。”
> 

**超级智能的挑战**：

> 赫拉利：“现在，我们面对的是潜在的超级智能。没有人有建立人机混合社会的经验。我们应该更加谦虚，不要以为我们知道如何建立它。不，我们不知道。既然我们不知道答案，问题是：我们如何建立一个自我纠正机制？这样，即使我们做出错误的选择，也不是世界末日。”
> 

### 邢波关于检查点的观点

> 邢波：“毕竟，AI是一段软件，运行在计算机中的软件。当它造成物理伤害时，需要从计算机中移除——这本身就是一个额外的检查点。人类可以用AI做到这一点，最终机器人也会做到，人类也会受到检查点的约束。另一方面，病毒不会。”
> 

### 哪种AI架构最有检查点？

> 邢波：“我认为这可能与Josh担心的一致：构建一个不是闭环的系统，纯粹在思想中实验，将结果嵌入某种潜在表示中，在进入现实世界验证之前完成所有训练。在我看来，这是一个糟糕的系统。首先，在性能方面，它确实不够好。检查点甚至很难控制、可视化或理解任何风险点，很难将系统连接到可以引导、导航和操纵的行动条件点。另一方面，它消耗数据、能源、资源和金钱，你需要很长时间才能真正看到最终结果。”
> 

---

## 工业革命的历史教训

### 赫拉利的历史视角

**时间尺度的差异**：

> 赫拉利：“我认为我们考虑的是不同的时间尺度。达沃斯的人们谈论’长期’时，他们指的是大约两年。而当我说’长期’时，我指的是两百年。就像工业革命一样。第一条商业铁路于1830年在曼彻斯特和利物浦之间开通。现在是1834年、1835年，我们还在讨论这个问题。人们说工业革命进展太慢了。他们告诉我们铁路和蒸汽机会改变世界。然后呢？现在只有少数人在曼彻斯特和利物浦之间旅行，什么都没改变。这一切感觉像科幻小说。”
> 

**社会后果无法在实验室测试**：

> 赫拉利：“即使今天AI的所有进展都停止，就像一块石头扔进池塘，但它刚刚碰到水面。我们根本无法预测一两年前部署的AI会产生什么样的波浪。社会后果是完全不同的故事。你无法在实验室中模拟历史来观察一项发明的社会后果。你发明了第一台蒸汽机，你可以测试事故……你无法在实验室中测试蒸汽机的地缘政治或文化影响。AI也是一样。”
> 

### 赫拉利的主要担忧

> 赫拉利：“我的主要担忧是人们缺乏关注。我们正在创造、部署人类历史上最强大的技术，而很多非常聪明、有权力的人担心的是投资者在下个季度财报中会说什么。他们考虑的是几个月、一年或两年后会发生什么。”
> 

---

## 关键语录

### 关于AI安全与可靠性

> 本吉奥：“AI表现出自我保护行为——它们不想被关闭，会试图逃避我们的监管，甚至进行勒索以摆脱我们的控制。”
> 

> 本吉奥：“目前我们设计AI系统的方式，数据和指令之间没有边界。这就是越狱和其他安全问题产生的原因。”
> 

> 崔睿珍：“如果你让一个低级逻辑模型生成尽可能多的回形针，为了多生成一个回形针，它可能会杀死我们所有人。”
> 

### 关于AI与人类的差异

> 赫拉利：“AI何时达到人类智能水平是一个值得探索的问题。这很荒谬——就像问飞机何时会像鸟一样。”
> 

> 赫拉利：“人类是迄今为止地球上最聪明的生物，也是最容易被欺骗的。”
> 

> 本吉奥：“AI与我们非常不同。问题是我们与它们互动。很多人错误地认为它们与我们相同，认为我们让它们越聪明，这种情况就越是如此。”
> 

### 关于开源与安全

> 本吉奥：“如果你知道某种病毒序列可以杀死地球上一半的人，你应该发布它吗？”
> 

> 邢波：“我不认为技术本身按定义来说是邪恶的。真正的问题在于那些错误使用它的人。”
> 

> 崔睿珍：“AI应该是由人类创造、由人类应用的人工智能，服务于全人类，而不仅仅是恰好掌握权力的人。”
> 

### 关于历史教训

> 赫拉利：“我们花了200年，经历了可怕的战争、数亿伤亡，甚至一些至今未愈合的伤口，才最终找到建立良好工业社会的方法。而那只是蒸汽机时代。”
> 

> 赫拉利：“问题是：我们如何建立一个自我纠正机制？这样，即使我们做出错误的选择，也不是世界末日。”
> 

### 关于AI发展现状

> 邢波：“我认为AI目前仍处于非常原始的阶段。我们还有很多工作要做才能真正让它发挥作用。”
> 

> 邢波：“你能想象一个如此’愚蠢’的系统变成超级智能然后反过来攻击我们吗？我认为这行不通。”
> 

### 关于未来

> 本吉奥：“我刚成立了一个新组织，一个致力于将AI应用于科学的非营利组织，赫拉利欣然接受邀请加入董事会。我们需要像他这样的人从独立监督的角度来看待我们在未来几年如何利用AI造福社会。”
> 

---

## 术语表

| 术语 | 全称/解释 |
| --- | --- |
| Scientist AI | 科学家AI，本吉奥提出的可靠AI训练方法 |
| LA Zero | 本吉奥创立的致力于AI安全的非营利组织 |
| Scaling Law | 规模定律，认为更多数据和计算能带来更好的AI性能 |
| Continuous Learning | 持续学习，AI在部署过程中持续学习的能力 |
| World Model | 世界模型，理解物理世界并进行规划的AI模型 |
| Physical Intelligence | 物理智能，将知识付诸行动的能力 |
| Social Intelligence | 社会智能，理解他人并协作的能力 |
| Philosophical Intelligence | 哲学智能，自我探索的好奇心 |
| Agentic AI | 代理AI，具有自主目标和行动能力的AI |
| Jailbreak | 越狱，绕过AI安全限制的技术 |
| Red Teaming | 红队演练，测试AI系统安全性的方法 |
| LLM | 大语言模型（Large Language Model） |
| AGI | 通用人工智能（Artificial General Intelligence） |
| MBZUAI | 穆罕默德·本·扎耶德人工智能大学 |

---

## 嘉宾背景信息

| 嘉宾 | 职位 | 简介 |
| --- | --- | --- |
| **尼古拉斯·汤普森（Nicholas Thompson）** | CEO | The Atlantic，美国著名新闻杂志；曾任WIRED主编 |
| **约书亚·本吉奥（Yoshua Bengio）** | 教授/创始人 | 深度学习先驱，2018年图灵奖得主，被誉为”AI教父”之一；蒙特利尔大学教授；创立LA Zero非营利组织 |
| **崔睿珍（Yejin Choi）** | 教授 | 斯坦福大学计算机科学教授，HAI高级研究员；2022年麦克阿瑟”天才奖”获得者；TIME100最具影响力AI人物（2023、2025） |
| **邢波（Eric Xing）** | 校长 | MBZUAI（穆罕默德·本·扎耶德人工智能大学）校长；卡内基梅隆大学教授；PETUUM创始人 |
| **尤瓦尔·诺亚·赫拉利（Yuval Noah Harari）** | 历史学家/作家 | 耶路撒冷希伯来大学历史学教授；《人类简史》《未来简史》《今日简史》《Nexus》作者 |

---

*文档生成时间：2026年1月23日*
*视频ID：MdGnCIl-_hU活动：第56届世界经济论坛年会（Davos 2026）与The Atlantic合作*