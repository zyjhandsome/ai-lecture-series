# 20251218 Crazy_Podcast_Google_DeepMind_Sebastian_Boro访谈_Gemini_3发展与AI团队实践_整理文档

# Gemini 3 发展与 AI 团队实践深度访谈

## 视频基本信息

| 项目 | 内容 |
| --- | --- |
| **视频标题** | 251218-Gemini 3发展与AI团队实践深度访谈 |
| **视频链接** | https://www.bilibili.com/video/BV1sCqqBuESy |
| **发布时间** | 2025年12月18日 |
| **视频时长** | 未提供 |
| **节目名称** | Crazy Podcast |
| **嘉宾** | Sebastian Boro（Google DeepMind Gemini 3 预训练负责人） |
| **主持人** | Matt Turk |
| **搬运UP主** | 皮皮蟹勇闯天涯 |

---

## 目录

1. [核心观点与预测](about:blank#%E6%A0%B8%E5%BF%83%E8%A7%82%E7%82%B9%E4%B8%8E%E9%A2%84%E6%B5%8B)
2. [Gemini 3 的核心秘密](about:blank#gemini-3-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%A7%98%E5%AF%86)
3. [AI 研究的现状与未来](about:blank#ai-%E7%A0%94%E7%A9%B6%E7%9A%84%E7%8E%B0%E7%8A%B6%E4%B8%8E%E6%9C%AA%E6%9D%A5)
4. [Gemini 3 的架构设计](about:blank#gemini-3-%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1)
5. [预训练数据与 Scaling Law](about:blank#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%8E-scaling-law)
6. [Sebastian Boro 的个人背景](about:blank#sebastian-boro-%E7%9A%84%E4%B8%AA%E4%BA%BA%E8%83%8C%E6%99%AF)
7. [研究团队的组织与管理](about:blank#%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%E7%9A%84%E7%BB%84%E7%BB%87%E4%B8%8E%E7%AE%A1%E7%90%86)
8. [研究品味与方法论](about:blank#%E7%A0%94%E7%A9%B6%E5%93%81%E5%91%B3%E4%B8%8E%E6%96%B9%E6%B3%95%E8%AE%BA)
9. [合成数据与数据约束](about:blank#%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BA%A6%E6%9D%9F)
10. [长上下文与模型架构创新](about:blank#%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0)
11. [AI 代理与 Agentic 应用](about:blank#ai-%E4%BB%A3%E7%90%86%E4%B8%8E-agentic-%E5%BA%94%E7%94%A8)
12. [持续学习与未来方向](about:blank#%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91)
13. [对初创公司和研究者的建议](about:blank#%E5%AF%B9%E5%88%9D%E5%88%9B%E5%85%AC%E5%8F%B8%E5%92%8C%E7%A0%94%E7%A9%B6%E8%80%85%E7%9A%84%E5%BB%BA%E8%AE%AE)
14. [关键语录](about:blank#%E5%85%B3%E9%94%AE%E8%AF%AD%E5%BD%95)
15. [术语表](about:blank#%E6%9C%AF%E8%AF%AD%E8%A1%A8)

---

## 核心观点与预测

| 观点/预测 | 提出者 | 详细说明 |
| --- | --- | --- |
| AI 进展已超出预期 | Sebastian Boro | 2019-2020年开始做语言模型研究时，没想到能达到今天的规模和能力 |
| 我们不只是在构建模型，而是在构建整个系统 | Sebastian Boro | Gemini 3 的进步是整个团队协作的结果，涉及模型、数据、基础设施等多个方面 |
| Scaling Law 仍然有效 | Sebastian Boro | 规模扩展是预训练的关键因素，但不是唯一因素 |
| 从无限数据到数据约束的范式转变 | Sebastian Boro | 这一转变正在改变研究方向和解决问题的方式 |
| 未来几年将有重大科学发现 | Sebastian Boro | AI 将在科学研究领域取得突破性进展 |
| 注意力机制方面有重要发现 | Sebastian Boro | 近期的发现将对未来几个月的研究产生深远影响 |
| 检索（Retrieval）研究时机已成熟 | Sebastian Boro | Retro 项目当时不成熟，但现在可能变得可行 |

---

## Gemini 3 的核心秘密

### 官方表述

Google DeepMind 深度学习研究总监、Gemini 项目联合负责人 Oriol Vinyals 在 Gemini 3 发布时提到，模型的核心秘密出奇地简单：**更好的预训练和后训练**。

### Sebastian 的解读

> “从我的角度来看，这确实相当普遍。”
> 
- **不存在单一的重大突破**：Gemini 3 的进步是多个小改进的组合和大型团队协作的结果
- **关键因素**：可能只有少数几个关键因素影响较大，但本质上是整个团队协作的成果
- **系统思维**：现在构建的不只是模型，而是整个系统架构

### 对 AI 进展的意义

1. **进步持续加速**：每天都能看到参数调整带来的改进，没有放缓迹象
2. **系统层面的突破**：不只是训练神经网络架构，而是构建整个系统架构
3. **智能评估**：应该关注模型的真正进步，而非单纯追求基准测试成绩

---

## AI 研究的现状与未来

### 当前位置评估

> Sebastian Boro：“如果我对自己诚实的话，我觉得我们已经超越了我认为可能的范围。”
> 
- 2019-2020 年开始做语言模型研究时，难以相信今天的规模和能力
- 回顾之前的扩展定律，它们确实指向了这个方向，但当时并不确信会实现
- 未来五年如果保持同样的进展速度，结果将非常惊人

### 模型能力提升的信心来源

1. **基准测试**：
    - 测试越来越难
    - 即使是计算机科学背景的人，有些问题也需要花很长时间回答
    - 但存在对过拟合和”刷榜”的担忧
2. **实际使用效果**：
    - 内部使用效率持续提高
    - 每一代新模型都能完成更多新任务
    - 在日常研究和工程中比上一代模型更有效

### AI 研究自动化

- **不是完全自动化，而是加速曲线**
- 更多时间可以投入到更高层次的研究部分
- 日常研究大量时间花在：运行实验、监控实验、分析数据收集
- 有趣的部分是：形成假设和设计新实验
- 明年随着更多 Agentic 工作流的启用，工作将显著加速

---

## Gemini 3 的架构设计

### 整体架构

- **混合专家（MoE）架构的 Transformer 模型**
- 包含原始 Transformer 论文中的许多组件
- 相比 Gemini 2.5，架构变化不大，主要是多个组件协同工作实现显著改进

### Transformer 核心模块

**1. 注意力模块（Attention）**
- 负责跨时间、跨不同 token 混合信息

**2. 前馈模块（Feedforward）**
- 为模型提供记忆和推理计算能力
- 在单个 token 上操作
- 这些模块并行运行

### MoE（混合专家）架构解释

原始 Transformer 架构中，前馈模块是密集计算过程：
- 输入线性转换到隐藏维度
- 应用激活函数
- 输出线性转换

MoE 的核心思想：
- **解耦计算资源和参数使用**
- 动态路由，决定将计算资源分配给哪个专家模型
- 不再耦合，而是动态分配

### 原生多模态

- Gemini 原生支持多模态
- **同一个神经网络处理所有不同模态**（文本、图像、视频）
- 没有专门处理图像的模型、专门处理文本的模型
- 这种方式带来两个成本考虑：
    1. **复杂性成本和研究投入**：需要考虑不同模态之间的交互
    2. **计算成本**：图像输入规模通常比文本大得多

---

## 预训练数据与 Scaling Law

### Scaling Law 的现状

> “我认为这些讨论总是有点奇怪，因为我的经验与此不符。”
> 
- 规模在预训练中仍然至关重要
- 规模的优势在于**相对可预测**
- 但规模不是唯一因素

### 影响预训练性能的关键因素

1. **规模（Scale）**：仍然是重要因素
2. **架构创新（Architecture）**：可能比单纯扩展规模更重要
3. **数据创新（Data）**：同样关键

### Chinchilla 模型的贡献

Sebastian 参与的 Chinchilla 项目重新审视了模型和数据的扩展方式：
- 从优化计算训练资源的角度出发
- 发现应该比之前更快地扩展数据侧
- 而不是只扩展模型
- 这一发现至今仍影响日常工作，特别是影响：
- 服务成本
- 训练后使用模型的成本

### 数据是否会枯竭？

> “我不这么认为，还有更多我们可以做的。”
> 

**范式转变**：
- 从**数据无限扩展阶段**转向**数据约束阶段**
- 这改变了研究方向和解决问题的方式
- 类似于 ImageNet 阶段之前，大量工作在数据受限环境中完成

---

## Sebastian Boro 的个人背景

### 成长经历

- 出生于荷兰
- 7 岁移居瑞士
- 父亲是瑞士人，母亲来自德国
- 在瑞士完成大部分小学和中学教育（主要用法语，部分德语）
- 15 岁移居意大利，19 岁完成高中
- 原计划去苏黎世大学，偶然发现剑桥在排名榜首，决定申请
- 在剑桥完成本科和硕士学位（计算机系）

### 职业发展

**2018 年加入 DeepMind**：
- 硕士期间有一位导师也是 DeepMind 研究员
- 课程结束时主动请求推荐信
- 获得面试机会并加入

**早期项目**：
- 最初在强化学习领域（DeepMind 当时以此闻名）
- 训练无监督网络在 Atari 环境中提取关键点
- 约 6 个月后转向表示学习（Representation Learning）

**对研究的偏好**：
> “我一直想用实际数据工作，产生更现实的影响。我喜欢构建东西，构建能用的东西。”

**大规模语言模型时期**：
1. **Gopher 项目**：DeepMind 发布的第一个语言模型论文
- 约 10-12 人的团队
- 训练了 2800 亿参数的密集 Transformer 模型
- 处理约 3000 亿 token

1. **Chinchilla 项目**：重新审视模型和数据扩展方式
2. **Retro 项目**：架构创新
    - 通过检索大量文本来提升模型能力
    - 让模型能够查询特定内容，而非将所有知识存储在参数中

---

## 研究团队的组织与管理

### Sebastian 的职责

作为 Gemini 3 预训练负责人之一：
1. **改进模型性能**：现在更少亲自做实验，更多是帮助设计实验和审查结果
2. **协调和整合**：团队约 150-200 人，负责日常预训练项目，涵盖数据、模型、基础设施

### 团队协作的重要性

> “最大的进步是让每个人高效地向前推进，而不是让少数人领先。”
> 
- 短期内让一两个人或小团队领先可能有效
- 长期真正的成功是让大多数人协同工作

### 预训练与后训练的组织

- 预训练团队：负责模型和数据
- 基础设施团队：非常重要，常被低估
- 后训练团队
- 服务团队

### 研究与产品的平衡

> “我从未真正感受到这种压力。”
> 
- Google 的领导层大多有研究背景
- 他们深知可以强制加速特定基准和目标，但最终还是要让研究工作发挥作用
- 研究与产品的张力只是同一主题的不同表现形式

---

## 研究品味与方法论

### 什么是研究品味（Research Taste）？

**核心要素一：系统协作**
- 研究不能独立存在
- 必须与他人的研究协同工作并有效整合
- 如果改进让其他人使用困难度增加 5%，可能不是好的权衡

**核心要素二：复杂性厌恶**
- 必须维持一定的复杂性预算
- 研究风险会累积
- 很多时候不追求性能最优版本，而是牺牲部分性能换取更低复杂度

### 实验直觉

- 与计算资源高度相关
- 如果计算资源更多，进展会更快
- 需要猜测哪个研究分支值得探索
- 必须找出正确的实验方向

### 何时放弃

- 大多数研究想法会失败
- 必须决定何时投入足够资源后转向其他方向
- 负面结果不意味着无效，通常只是意味着不成功
- 需要意识到这一点并做出判断

### 短期与长期的平衡

**短期工作**：
- 解决模型需要改进的关键路径
- 修复看似不完美的部分（后续往往会出问题）

**长期探索**：
- 可能在下一版 Gemini 或更晚才有更大改进
- 尚未完全验证的想法

**平衡策略**：
- 扩展期间通常有更多探索性研究
- 准备升级新架构或新模型前，高度聚焦执行

---

## 合成数据与数据约束

### 合成数据的使用原则

> “合成数据很有趣，必须非常谨慎地使用，因为很容易用错。”
> 

**常见问题**：
- 用强模型生成合成数据
- 进行小规模验证
- 但真正有趣的问题是：合成数据能否训练出超越原始生成模型的未来模型？

### 数据约束的范式转变

- 从数据无限扩展阶段转向数据约束阶段
- 这改变了研究方向和问题解决方式
- 类似 ImageNet 之前的数据受限环境，大量技术开始变得有趣

### 模型从更少数据中学习

- 架构研究正是在解决这个问题
- 架构改进通常意味着：用相同数据量获得更好结果，或用更少数据获得同样结果
- 目前模型需要的数据量仍远超人类所能获取的数据量

---

## 长上下文与模型架构创新

### Gemini 1.5 的突破

- 在长上下文能力方面实现巨大飞跃
- 对当前模型和代理执行需要处理整个代码库的工作非常重要

### 未来方向

- 未来一两年在长上下文方面会有更多创新
- 更高效地扩展模型上下文长度
- **注意力机制方面最近有重要发现**，将对未来几个月的研究产生深远影响

### 检索（Retrieval）的未来

> “这就是我当初做 Retro 项目时关注的核心问题。”
> 

**为什么 Retro 当时不成熟**：
- 预训练阶段的迭代循环比现在慢得多
- 大规模调优风险高、耗时长
- RAG 或搜索可以在微调阶段实现，迭代更快

**未来方向**：
- 在预训练阶段以可微分方式学习
- 将检索能力集成到训练过程中
- 这将是未来几年的重要方向

---

## AI 代理与 Agentic 应用

### 与预训练的关系

- **感知和视觉方面非常重要**
- 模型需要与计算机屏幕交互
- 良好的屏幕理解能力是预训练阶段的重要部分

### Anti-Gravity（Google AI IDE）

- 涉及完整的编码元素
- 某种程度上，“感觉”与预训练相关
- 在 Vibe Coding（直觉编程）场景中可能更多涉及 RL 扩展和后训练

---

## 持续学习与未来方向

### 什么是持续学习？

- 随着新知识发现而更新模型
- 假设明天有新的科学突破，昨天训练的基础模型可能不知道这些

### 当前进展

**后训练方向**：
- 搜索应用和工具设置
- 让模型能够获取新信息
- 类似 Retro 的检索方式，外化知识库和推理部分

**长上下文方向**：
- 不断扩展用户上下文
- 让模型持续获取更多上下文信息
- 这是持续学习能力的一部分

### 令人兴奋的研究领域

1. **长上下文架构及相关研究**
2. **注意力机制**
3. **数据创新**：从无限数据到有限数据环境
4. **推理效率**：模型在服务和部署时的成本
5. **评估指标改进**

---

## 对初创公司和研究者的建议

### 对博士生的建议

> “理解我们正在构建的系统层面的知识越来越重要。”
> 

**关键能力**：
- 理解整个技术栈如何工作，从 TPU 到研究
- 能够发现层与层之间的差距
- 能够将研究想法的意义一直推导到 TPU 栈
- 研究工程和系统级模型研究，而非纯粹的模型架构研究

**推荐方向**：
- 检索研究（Retro 类型的研究）
- 当时不成熟，但现在正在改变
- 未来几年可能对 Gemini 这样的领先模型变得可行

### 对初创公司的建议

> “看看模型一年半前能做什么，再看看现在能做什么，尝试推断趋势。”
> 

**建议**：
- 模型正在改进的领域将继续改进
- 某些可能进展不大的领域可能更值得研究
- 思考通用模型可能在半年或一年后覆盖的任务

### 行业投资方向

> “可能两年前还存在研究与实际投资的脱节，现在已经改善很多。”
> 
- 之前人们还在创建专用模型来解决通用模型半年后就能覆盖的任务
- 现在人们已经意识到这一点
- 对于不需要极端专业化模型的通用任务，尝试使用通用模型

---

## 关键语录

### 关于 AI 进展

> Sebastian Boro：“如果我对自己诚实的话，我觉得我们已经超越了我认为可能的范围。”
> 

> Sebastian Boro：“我们不只是在构建模型，现在我们在构建整个系统。”
> 

### 关于研究品味

> Sebastian Boro：“很多时候我们不追求性能最优版本的研究想法，而是牺牲部分性能换取更低复杂度版本，因为我们认为这会在未来带来更多进步。”
> 

### 关于数据

> Sebastian Boro：“我们正处于一个范式转变中，从数据无限扩展阶段转向数据约束阶段，这实际上改变了很多研究方向和我们思考问题的方式。”
> 

### 关于团队协作

> Sebastian Boro：“最大的进步是让每个人高效地向前推进，而不是让少数人领先。短期内让一两个人或小团队领先可能有效，但长期真正的成功是让大多数人协同工作。”
> 

### 关于评估

> Sebastian Boro：“外部基准测试可以临时使用，但它们会很快被污染，开始以不同形式被复制……避免自欺欺人的唯一方法是创建保留测试集并真正保留它们。”
> 

### 关于职业发展

> Sebastian Boro：“我最喜欢的是每天与很多人一起工作，向众多研究人员学习。每天来上班，与真正出色的人交谈，他们教我以前不知道的东西。”
> 

### 关于未来

> Sebastian Boro：“我不觉得这条研究路径有尽头，这种工作将继续推动我们前进……至少在未来一年左右，我没有看到任何放缓的迹象。”
> 

---

## 术语表

| 术语 | 全称/解释 |
| --- | --- |
| **Gemini** | Google DeepMind 开发的多模态 AI 模型系列 |
| **MoE** | Mixture of Experts，混合专家架构 |
| **Transformer** | 一种神经网络架构，基于自注意力机制 |
| **Scaling Law** | 规模定律，描述模型性能与计算、数据、参数之间的关系 |
| **Chinchilla** | DeepMind 的研究项目，重新审视数据和模型规模的最优比例 |
| **Retro** | DeepMind 的检索增强模型项目 |
| **TPU** | Tensor Processing Unit，Google 开发的 AI 加速芯片 |
| **RAG** | Retrieval-Augmented Generation，检索增强生成 |
| **RLHF** | Reinforcement Learning from Human Feedback，基于人类反馈的强化学习 |
| **Agentic** | 代理式的，指 AI 系统具有自主执行任务的能力 |
| **Gopher** | DeepMind 发布的大型语言模型 |
| **BERT** | Bidirectional Encoder Representations from Transformers，Google 的双向编码器模型 |
| **XLNet** | 一种结合自回归和自编码的预训练模型 |
| **Anti-Gravity** | Google 发布的 AI IDE |
| **Vibe Coding** | 直觉编程，一种利用 AI 辅助的编程方式 |
| **长上下文** | Long Context，模型处理长文本序列的能力 |
| **持续学习** | Continuous Learning，模型能够不断更新知识的能力 |

---

*文档生成时间：2026年1月30日视频ID：BV1sCqqBuESy*